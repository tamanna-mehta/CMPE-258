{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tamanna_Hw-3_NN(partb2_final).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uR0jCLulIxy6"
      },
      "source": [
        "# part b-1) Neural network with pytorch (basic) with auto differentiation\n",
        "We are going to make neural network with 3 inputs and 2 outputs. Relu function is used for non linearity at each layer. 2 hidden layers are used -one consisting of 5 neurons and second hidden layer consisting of four neurons.We are tryint to predict more than one value.\n",
        "we use dtwo non linear functions-\n",
        "1. 5x1^2+x2^2+x3^2+5x1+x2+x3\n",
        "2. 4x1^2+x2^2+2x3^2+4x1+x2+2x3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1HhMnc2bK60n"
      },
      "source": [
        "import numpy as np\n",
        "import random\n",
        "from numpy import array\n",
        "from numpy.random import uniform\n",
        "from numpy import hstack\n",
        "import torch\n",
        "import torch.nn as nn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-XMtxnEW7jPX"
      },
      "source": [
        "#creating a class to define linear layers , here input dim represents number of input attributes, num_hidden is number of output neurons  \n",
        "class Linear():\n",
        "  def __init__(self,input_dim:int,num_hidden:int):\n",
        "    self.weights=torch.randn(input_dim, num_hidden, requires_grad=True)\n",
        "    self.bias=torch.zeros(num_hidden)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    self.x = x\n",
        "    output = self.x.mm(self.weights) + self.bias\n",
        "    return output\n",
        "\n",
        "\n",
        "  def updateWeights(self, lr):\n",
        "    self.weights -= lr * self.weights.grad\n",
        "      \n",
        "  def gradZero(self):\n",
        "    with torch.no_grad():\n",
        "      # Manually zero the gradients after running the backward pass\n",
        "      self.weights.grad.zero_()\n",
        "  \n",
        "  # def backward(self, gradient): # calculating loss function gradient\n",
        "  #   self.weights_gradient = self.x.T.mm(gradient) \n",
        "  #   self.bias_gradient = gradient.sum(axis=0)\n",
        "  #   self.x_gradient = gradient.mm(self.weights.T)\n",
        "  #   return self.x_gradient\n",
        "\n",
        "  # def update(self, lr): # updating weights and bias\n",
        "  #   self.weights = self.weights - lr * self.weights_gradient\n",
        "  #   self.bias = self.bias - lr * self.bias_gradient\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3uAneMiI2iP"
      },
      "source": [
        "#defining relu function and its derative\n",
        "class Relu():\n",
        "  def __call__(self,input_):\n",
        "    self.input_=input_\n",
        "    self.output=self.input_.clamp(min=0)\n",
        "   # print(input_,self.output)#torch.clamp(self.input_, min=0)#\n",
        "    return self.output\n",
        "  def backward(self,output_gradient):\n",
        "    self.input_gradient=(self.input_>0)*output_gradient\n",
        "    return self.input_gradient\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sd8arTPSJiVc"
      },
      "source": [
        "#creating Non linear model consisting of 3 layers-2 hidden and one output layer consisting of two output neurons.\n",
        "class TorchModel():\n",
        "   def __init__(self,input_dim:int,num_hidden1:int,num_hidden2:int):\n",
        "    #super().__init__()\n",
        "    self.linear1=Linear(input_dim, num_hidden1)\n",
        "    self.relu1=Relu()\n",
        "    self.linear2=Linear(num_hidden1,num_hidden2)\n",
        "    self.relu2=Relu()\n",
        "    self.linear3=Linear(num_hidden2,2)\n",
        "  \n",
        "   def __call__(self,x): # forward propagation step\n",
        "    l1=self.linear1(x)\n",
        "    r1=self.relu1(l1)\n",
        "    l2=self.linear2(r1)\n",
        "    r2=self.relu2(l2)\n",
        "    l3=self.linear3(r2)\n",
        "    return l3\n",
        "\n",
        "   def updateWeights(self, lr):\n",
        "    with torch.no_grad():\n",
        "      self.linear1.updateWeights(lr)\n",
        "      self.linear2.updateWeights(lr)\n",
        "      self.linear3.updateWeights(lr)\n",
        "\n",
        "      self.linear1.gradZero()\n",
        "      self.linear2.gradZero()\n",
        "      self.linear3.gradZero()\n",
        "\n",
        "\n",
        "\n",
        "  #  def backward(self,output_gradient): # Backward Propagation step\n",
        "  #   linear3_gradient=self.linear3.backward(output_gradient)\n",
        "  #   relu_gradient1=self.relu2.backward(linear3_gradient)\n",
        "  #   linear2_gradient=self.linear2.backward(relu_gradient1)\n",
        "  #   relu_gradient2=self.relu1.backward(linear2_gradient)\n",
        "  #   linear1_gradient=self.linear1.backward(relu_gradient2)\n",
        "  #   return linear1_gradient\n",
        "  \n",
        "  #  def update(self,lr): # updating weights for each layer\n",
        "  #   self.linear3.update(lr)\n",
        "  #   self.linear2.update(lr)\n",
        "  #   self.linear1.update(lr)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDFT2xKuh5td"
      },
      "source": [
        "#defining loss function-mse and its derivative  \n",
        "class MSE:\n",
        "  def __call__(self,y_pred,y_true):\n",
        "    self.y_pred=y_pred\n",
        "    self.y_true=y_true\n",
        "    return (y_pred-y_true).pow(2).mean()\n",
        "  # def backward(self):\n",
        "  #  n=self.y_true.shape[0]\n",
        "  #  self.gradient=2.*(self.y_pred-self.y_true)/n\n",
        "  #  return self.gradient"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-qk_vj-kSkd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70b0d91f-c7f1-4997-9a32-28948247da24"
      },
      "source": [
        "n=400  # number of input samples\n",
        "d=3    # number of dimensions\n",
        "num_hidden1=5 # number of neurons for first hidden layer\n",
        "num_hidden2=4 # numbe rof neurons for second hidden layer\n",
        "x=np.random.uniform(-1,1,(n,d)) # generating data of size=400 X 3\n",
        "x_tensor=torch.tensor(x,dtype=torch.float32)\n",
        "\n",
        "w_true=torch.tensor([[5,1,1],[4,1,1]], dtype=torch.float32).T\n",
        "bias_true=torch.tensor([1], dtype=torch.float32)\n",
        "\n",
        "y=torch.square(x_tensor).mm(w_true)+x_tensor.mm(w_true)+bias_true\n",
        "\n",
        "print(f'x: {x.shape},weights:{w_true.shape}, bias: {bias_true.shape},y:{y.shape}')\n",
        "loss=MSE() \n",
        "model=TorchModel(d,num_hidden1,num_hidden2) \n",
        "num_epochs=300\n",
        "lr=0.01\n",
        "for epoch in range(num_epochs):\n",
        "    \n",
        "    y_pred_tensor=model(x_tensor)\n",
        "    loss_value=loss(y_pred_tensor,y)\n",
        "    if(epoch%5==0):\n",
        "      print(f'Epoch {epoch}, loss{loss_value}')\n",
        "    \n",
        "    loss_value.backward()\n",
        "    model.updateWeights(lr)\n",
        "    \"\"\"\n",
        "    with torch.no_grad():\n",
        "      w_true -= lr * w_true.grad\n",
        "      #w2_true -= lr * w2_true.grad\n",
        "      # Manually zero the gradients after running the backward pass\n",
        "      w_true.grad.zero_()\n",
        "      #w2_true.grad.zero_()\n",
        "    \"\"\"\n",
        "# torch_fit(x_tensor,y_true_tensor, model=model, loss=loss, lr=0.007, num_epochs=55,w1_true,w2_true) # running the non linear model\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x: (400, 3),weights:torch.Size([3, 2]), bias: torch.Size([1]),y:torch.Size([400, 2])\n",
            "Epoch 0, loss23.709980010986328\n",
            "Epoch 5, loss18.229494094848633\n",
            "Epoch 10, loss15.066071510314941\n",
            "Epoch 15, loss12.844260215759277\n",
            "Epoch 20, loss10.843271255493164\n",
            "Epoch 25, loss8.94902515411377\n",
            "Epoch 30, loss7.119389057159424\n",
            "Epoch 35, loss5.434210777282715\n",
            "Epoch 40, loss4.1667633056640625\n",
            "Epoch 45, loss3.243192195892334\n",
            "Epoch 50, loss2.6033143997192383\n",
            "Epoch 55, loss2.147749662399292\n",
            "Epoch 60, loss1.8174667358398438\n",
            "Epoch 65, loss1.5701411962509155\n",
            "Epoch 70, loss1.3811525106430054\n",
            "Epoch 75, loss1.2368693351745605\n",
            "Epoch 80, loss1.1237913370132446\n",
            "Epoch 85, loss1.028102159500122\n",
            "Epoch 90, loss0.9461012482643127\n",
            "Epoch 95, loss0.8784114718437195\n",
            "Epoch 100, loss0.8220407366752625\n",
            "Epoch 105, loss0.7717104554176331\n",
            "Epoch 110, loss0.726370096206665\n",
            "Epoch 115, loss0.6849358081817627\n",
            "Epoch 120, loss0.6477522850036621\n",
            "Epoch 125, loss0.612626314163208\n",
            "Epoch 130, loss0.5813928246498108\n",
            "Epoch 135, loss0.5534788370132446\n",
            "Epoch 140, loss0.5275845527648926\n",
            "Epoch 145, loss0.5045405626296997\n",
            "Epoch 150, loss0.48192328214645386\n",
            "Epoch 155, loss0.46165114641189575\n",
            "Epoch 160, loss0.44357040524482727\n",
            "Epoch 165, loss0.4272361397743225\n",
            "Epoch 170, loss0.412556916475296\n",
            "Epoch 175, loss0.3993433117866516\n",
            "Epoch 180, loss0.3872639536857605\n",
            "Epoch 185, loss0.37624022364616394\n",
            "Epoch 190, loss0.36623668670654297\n",
            "Epoch 195, loss0.3571108877658844\n",
            "Epoch 200, loss0.348536878824234\n",
            "Epoch 205, loss0.34058335423469543\n",
            "Epoch 210, loss0.333261102437973\n",
            "Epoch 215, loss0.32651287317276\n",
            "Epoch 220, loss0.32029396295547485\n",
            "Epoch 225, loss0.3145798146724701\n",
            "Epoch 230, loss0.3092917799949646\n",
            "Epoch 235, loss0.3043888807296753\n",
            "Epoch 240, loss0.2998350262641907\n",
            "Epoch 245, loss0.29559439420700073\n",
            "Epoch 250, loss0.2916161119937897\n",
            "Epoch 255, loss0.28786003589630127\n",
            "Epoch 260, loss0.28430721163749695\n",
            "Epoch 265, loss0.28084126114845276\n",
            "Epoch 270, loss0.27756989002227783\n",
            "Epoch 275, loss0.2744477093219757\n",
            "Epoch 280, loss0.27147048711776733\n",
            "Epoch 285, loss0.26860949397087097\n",
            "Epoch 290, loss0.2659062445163727\n",
            "Epoch 295, loss0.26335418224334717\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SRuFIFpquRHZ"
      },
      "source": [
        "\n",
        "def plot_intereactive_3d(x, y, y_pred=None):\n",
        "  import plotly.graph_objects as go\n",
        "\n",
        "  fig = go.Figure()\n",
        "  fig.add_trace(go.Scatter3d(x = x[:,0],\n",
        "                    y = x[:,1],\n",
        "                    z = y.reshape([-1]),\n",
        "                    opacity=0.5, mode='markers', name='Underlying Function'\n",
        "                    ))\n",
        " \n",
        "  if y_pred is not None:\n",
        "    fig.add_trace(go.Scatter3d(x = x[:,0],\n",
        "                   y = x[:,1],\n",
        "                   z = y_pred.reshape([-1]),\n",
        "                   opacity=0.5, mode='markers', name='Predicted Function'\n",
        "                  ))\n",
        "    \n",
        "  fig.update_layout(scene = dict(\n",
        "                    xaxis_title='X1',\n",
        "                    yaxis_title='X2',\n",
        "                    zaxis_title='Y'),\n",
        "                    width=700,\n",
        "                    margin=dict(r=20, b=10, l=10, t=10))\n",
        "  fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iUUIQFdCqOat"
      },
      "source": [
        "from sklearn.manifold import TSNE\n",
        "X_red = TSNE(n_components=2).fit_transform(x_tensor)\n",
        "y_true_red = TSNE(n_components=1).fit_transform(y)\n",
        "y_pred_red = TSNE(n_components=1).fit_transform(y_pred.detach())\n",
        "print(f'X_red: {X_red.shape}, y_true_red: {y_true_red.shape}, y_pred_red: {y_pred_red.shape}')\n",
        "\n",
        "plot_intereactive_3d(X_red,y_true_red,y_pred_red)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}